{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "long_context_model_name = \"gradientai/Llama-3-8B-Instruct-262k\" # Long Context Model\n",
    "base_model_name = \"kuotient/Meta-Llama-3-8B-Instruct\" # Base Model // Identical weight to official repo(gated)\n",
    "target_model_name = \"\"  # Model that want to expand context\n",
    "long_context_model = AutoModelForCausalLM.from_pretrained(long_context_model_name)\n",
    "base_model_name = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "def calculate_weight_diff(base_weight, chat_weight):\n",
    "    return torch.abs(base_weight - chat_weight).mean().item()\n",
    "\n",
    "def calculate_layer_diffs(long_context_model, base_model_name):\n",
    "    layer_diffs = []\n",
    "    for long_context_layer, base_model_layer in zip(long_context_model.model.layers, base_model_name.model.layers):\n",
    "        layer_diff = {\n",
    "            'input_layernorm': calculate_weight_diff(long_context_layer.input_layernorm.weight, base_model_layer.input_layernorm.weight),\n",
    "            'mlp_down_proj': calculate_weight_diff(long_context_layer.mlp.down_proj.weight, base_model_layer.mlp.down_proj.weight),\n",
    "            'mlp_gate_proj': calculate_weight_diff(long_context_layer.mlp.gate_proj.weight, base_model_layer.mlp.gate_proj.weight),\n",
    "            'mlp_up_proj': calculate_weight_diff(long_context_layer.mlp.up_proj.weight, base_model_layer.mlp.up_proj.weight),\n",
    "            'post_attention_layernorm': calculate_weight_diff(long_context_layer.post_attention_layernorm.weight, base_model_layer.post_attention_layernorm.weight),\n",
    "            'self_attn_q_proj': calculate_weight_diff(long_context_layer.self_attn.q_proj.weight, base_model_layer.self_attn.q_proj.weight),\n",
    "            'self_attn_k_proj': calculate_weight_diff(long_context_layer.self_attn.k_proj.weight, base_model_layer.self_attn.k_proj.weight),\n",
    "            'self_attn_v_proj': calculate_weight_diff(long_context_layer.self_attn.v_proj.weight, base_model_layer.self_attn.v_proj.weight),\n",
    "            'self_attn_o_proj': calculate_weight_diff(long_context_layer.self_attn.o_proj.weight, base_model_layer.self_attn.o_proj.weight)\n",
    "        }\n",
    "        layer_diffs.append(layer_diff)\n",
    "    return layer_diffs\n",
    "\n",
    "def apply_layer_diffs(target_model, layer_diffs):\n",
    "    for layer, layer_diff in zip(target_model.model.layers, layer_diffs):\n",
    "        layer.input_layernorm.weight.data += layer_diff['input_layernorm']\n",
    "        layer.mlp.down_proj.weight.data += layer_diff['mlp_down_proj']\n",
    "        layer.mlp.gate_proj.weight.data += layer_diff['mlp_gate_proj']\n",
    "        layer.mlp.up_proj.weight.data += layer_diff['mlp_up_proj']\n",
    "        layer.post_attention_layernorm.weight.data += layer_diff['post_attention_layernorm']\n",
    "        layer.self_attn.q_proj.weight.data += layer_diff['self_attn_q_proj']\n",
    "        layer.self_attn.k_proj.weight.data += layer_diff['self_attn_k_proj']\n",
    "        layer.self_attn.v_proj.weight.data += layer_diff['self_attn_v_proj']\n",
    "        layer.self_attn.o_proj.weight.data += layer_diff['self_attn_o_proj']\n",
    "\n",
    "def visualize_layer_diffs(layer_diffs):\n",
    "    num_layers = len(layer_diffs)\n",
    "    num_components = len(layer_diffs[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_components, figsize=(24, 8))\n",
    "    fig.suptitle(f\"{long_context_model_name} <> {base_model_name}\", fontsize=16)\n",
    "    \n",
    "    for i, component in enumerate(layer_diffs[0].keys()):\n",
    "        component_diffs = [[layer_diff[component]] for layer_diff in layer_diffs]\n",
    "        sns.heatmap(component_diffs, annot=True, fmt=\".6f\", cmap=\"YlGnBu\", ax=axs[i], cbar_kws={\"shrink\": 0.8})\n",
    "        axs[i].set_title(component)\n",
    "        axs[i].set_xlabel(\"Layer\")\n",
    "        axs[i].set_ylabel(\"Difference\")\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks(range(num_layers))\n",
    "        axs[i].set_yticklabels(range(num_layers))\n",
    "        axs[i].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# get diff\n",
    "layer_diffs = calculate_layer_diffs(long_context_model, base_model_name)\n",
    "\n",
    "# apply diff\n",
    "target_model = AutoModelForCausalLM.from_pretrained(long_context_model_name)\n",
    "apply_layer_diffs(target_model, layer_diffs)\n",
    "\n",
    "# save\n",
    "target_model.save_pretrained('./expanded_target')\n",
    "\n",
    "# wanna see diff?\n",
    "visualize_layer_diffs(layer_diffs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
